{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Magicfenixx/BachelorsThesisAI/blob/main/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3rtkb1GvstoV"
      },
      "outputs": [],
      "source": [
        "!pip install visualkeras\n",
        "!pip install optuna\n",
        "!pip install tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpfRbMgrxFAf"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import os\n",
        "import tensorflow as tf\n",
        "np.random.seed(2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.optimizers import RMSprop,AdamW\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eUqwQmSC3NTp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPf-roixJ4w"
      },
      "source": [
        "# SPLIT THE SCENE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVaA_jZqxGPd"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# input and output folder paths\n",
        "input_folder = '/content/input_images'\n",
        "output_folder = '/content/output_images'\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Function to split an image into 80x80 fragments\n",
        "def split_image(image_path, output_folder, size=(80, 80)):\n",
        "    img = Image.open(image_path)  # Open the image\n",
        "    img_width, img_height = img.size  # Get image dimensions\n",
        "    img_name = os.path.basename(image_path).split('.')[0]  # Get the image name\n",
        "\n",
        "    # Iterate through the image and crop it into 80x80 tiles\n",
        "    for i in range(0, img_width, size[0]):\n",
        "        for j in range(0, img_height, size[1]):\n",
        "            # Define the cropping box (left, upper, right, lower)\n",
        "            box = (i, j, i + size[0], j + size[1])\n",
        "            cropped_img = img.crop(box)  # Crop the image\n",
        "\n",
        "            # Save each tile with a unique name\n",
        "            cropped_img.save(os.path.join(output_folder, f'{img_name}_{i}_{j}.png'))\n",
        "\n",
        "# Loop through each image and split\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        split_image(image_path, output_folder)\n",
        "\n",
        "print('Images have been successfully split and saved!')\n",
        "\n",
        "output_folder = '/content/train/no_ships' # Path to the output folder\n",
        "shutil.make_archive('/content/output_images_zip', 'zip', output_folder) # Create a zip file of the output folder\n",
        "files.download('/content/output_images_zip.zip') # Download the zip file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ucYRAE4xL8r"
      },
      "source": [
        "# DOWNLOAD OF SECTORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGDQZyM7xJQp"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Path to the output folder\n",
        "output_folder = '/content/train/no_ships'\n",
        "\n",
        "# Create a zip file of the output folder\n",
        "shutil.make_archive('/content/output_images_zip', 'zip', output_folder)\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/output_images_zip.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgshqN4oxRoH"
      },
      "source": [
        "# REVIEW OF DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39136dMcxTJE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the folder containing 80x80 images\n",
        "fragments_folder = '/content/train/no_ships'  # Replace with your folder containing the 80x80 fragments\n",
        "\n",
        "# List all the images in the fragments folder\n",
        "fragment_files = [f for f in os.listdir(fragments_folder) if f.endswith('.png') or f.endswith('.jpg')]\n",
        "\n",
        "# Check if there are any fragments available\n",
        "if not fragment_files:\n",
        "    print('No fragments found in the folder!')\n",
        "else:\n",
        "    # Select a random image from the folder\n",
        "    random_fragment = random.choice(fragment_files)\n",
        "\n",
        "    # Open the image\n",
        "    fragment_path = os.path.join(fragments_folder, random_fragment)\n",
        "    img = Image.open(fragment_path)\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # Hide axis\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Displaying: {random_fragment}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM2lPZKQxNsb"
      },
      "source": [
        "# LABELING INITIAL DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b-F5KaCxPJm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "ship_folder = '/content/drive/MyDrive/train/ships'\n",
        "no_ship_folder = '/content/drive/MyDrive/train/no_ships'\n",
        "\n",
        "# Output CSV file\n",
        "output_csv = 'labeled_images.csv'\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(output_csv, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"filename\", \"label\"])  # Header row\n",
        "\n",
        "    # Label images in the ship folder (label = \"1\")\n",
        "    for img_file in os.listdir(ship_folder):\n",
        "        if img_file.endswith('.jpg') or img_file.endswith('.png'):\n",
        "            writer.writerow([os.path.join('ships', img_file), \"1\"])\n",
        "\n",
        "    # Label images in the no ship folder (label = \"0\")\n",
        "    for img_file in os.listdir(no_ship_folder):\n",
        "        if img_file.endswith('.jpg') or img_file.endswith('.png'):\n",
        "            writer.writerow([os.path.join('no_ships', img_file), \"0\"])\n",
        "\n",
        "print(f\"Labels saved to {output_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fEwO1t_xXMw"
      },
      "source": [
        "# UPDATING DICTIONARY OF LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlZLJQ4OxUre"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/labeled_images.csv')\n",
        "\n",
        "# Replace numerical labels with string labels\n",
        "df['label'] = df['label'].replace({1: 'ship', 0: 'no_ship'})\n",
        "\n",
        "# Save\n",
        "df.to_csv('/content/labeled_images.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2C8pUiwxce4"
      },
      "source": [
        "# TRAIN/VALIDATION SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBD83n5L-qUY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "k = 5  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Prepare cross-validation storage\n",
        "fold_accuracies = []  # Store accuracies for each fold\n",
        "\n",
        "df = pd.read_csv('/content/labeled_images.csv')  # Original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ2Qi-_mFWcM"
      },
      "outputs": [],
      "source": [
        "print(train_df.head())\n",
        "print(val_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ZUe6fexrxR"
      },
      "source": [
        "# AUGMENTATION OF DATA SINCE THE AMOUNT OF SHIPS ARE SMALL ( OLD MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ1mh1AsxtmE"
      },
      "source": [
        "INITIALLIZING IMAGE GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT72UmjPxseN"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgiwbUI2bLF1"
      },
      "outputs": [],
      "source": [
        "# def load_and_preprocess_image(file_path):\n",
        "#     image_size = 80\n",
        "#     image = tf.io.read_file(file_path)\n",
        "#     image = tf.image.decode_png(image, channels=1)  # Grayscale, channels=1\n",
        "#     image = tf.image.resize(image, [image_size, image_size])\n",
        "#     image = image / 255.0  # Normalize to [0, 1]\n",
        "#     return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c2vot1rcXrj"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     # rotation_range=0,      # Rotate images up to 30 degrees\n",
        "#     # width_shift_range=0,  # Horizontal shift\n",
        "#     # height_shift_range=0, # Vertical shift\n",
        "#     # shear_range=0,        # Shear transformation\n",
        "#     # zoom_range=0,         # Zoom in/out\n",
        "#     horizontal_flip=True,   # Randomly flip images horizontally\n",
        "#     vertical_flip=True,     # Randomly flip images vertically\n",
        "#     #fill_mode='nearest',     # Filling pixels after transformation\n",
        "#     rescale=1./255\n",
        "# )\n",
        "\n",
        "\n",
        "# # For validation/testing, don't use augmentations\n",
        "# test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8VYkv4Cxz_W"
      },
      "source": [
        "Utilizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wucJmLojxxsd"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# # Load your training and testing CSV files\n",
        "# train_df = pd.read_csv('/content/train_labels.csv')\n",
        "# test_df = pd.read_csv('/content/test_labels.csv')\n",
        "\n",
        "# # Path to your main directory where both 'ships' and 'no_ships' are located\n",
        "# image_directory = '/content/drive/MyDrive/train/'  # Main directory\n",
        "\n",
        "# # Training generator\n",
        "# train_generator = datagen.flow_from_dataframe(\n",
        "#     dataframe=train_df,\n",
        "#     directory=image_directory,  # Main directory containing both 'ships' and 'no_ships'\n",
        "#     x_col='filename',\n",
        "#     y_col='label',\n",
        "#     target_size=(80, 80),\n",
        "#     batch_size=115,\n",
        "#     class_mode='binary',\n",
        "#     color_mode='grayscale'  # This ensures the images are read as grayscale\n",
        "# )\n",
        "\n",
        "# # Testing/validation generator\n",
        "# test_generator = test_datagen.flow_from_dataframe(\n",
        "#     dataframe=test_df,\n",
        "#     directory=image_directory,  # Main directory for images\n",
        "#     x_col='filename',\n",
        "#     y_col='label',\n",
        "#     target_size=(80, 80),\n",
        "#     batch_size=115,\n",
        "#     class_mode='binary',\n",
        "#     color_mode='grayscale'  # This ensures the images are read as grayscale\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhtvOhFAresB"
      },
      "source": [
        "# TRAINING PREREQUISITES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFPbqsgzrkqR"
      },
      "outputs": [],
      "source": [
        "\"\"\" Balance classes (Old Model) \"\"\"\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                  classes=np.unique(train_df['label']),\n",
        "                                                  y=train_df['label'])\n",
        "\n",
        "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}  # Map 0 for 'no_ship' and 1 for 'ship'\n",
        "\n",
        "print(\"Class weights: \", class_weights_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nU1XNV2rkqS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "learning_rate_callback = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',  # Track validation loss\n",
        "    factor=0.50,          # Reduce learning rate by half when no improvement\n",
        "    patience=3,          # Wait for 3 epochs of no improvement\n",
        "    min_lr=1e-6,         #floor for the learning rate\n",
        "    verbose=1            # Display updates in output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br6mydRPHSXJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',           # Monitor the validation loss\n",
        "    patience=5,                   # Stop after 3 epochs with no improvement\n",
        "    restore_best_weights=True     # Restore the model to the best weights\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw3PrW5qrkqS"
      },
      "outputs": [],
      "source": [
        "\"\"\" Learning Rate Scheduler (Old model) \"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    warmup_epochs = 3\n",
        "    if epoch < warmup_epochs:\n",
        "        # Linear warmup for the first few epochs\n",
        "        return float(lr * (epoch + 1) / warmup_epochs)\n",
        "    else:\n",
        "        # Exponential decay after warmup\n",
        "        return float(lr * tf.math.exp(-0.1 * (epoch - warmup_epochs)))\n",
        "\n",
        "# Initialize learning rate scheduler callback\n",
        "warmup_lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEDxzUyIr9JT"
      },
      "source": [
        "# KERAS MODEL INITIAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8pFvL7ur9JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Clear previous models from memory\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgOIl71br9JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import RMSprop, AdamW\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "# Initialize the model\n",
        "Boat_Detection = Sequential()\n",
        "\n",
        "# layers\n",
        "Boat_Detection.add(Conv2D(32, (7,7), strides=(1, 1), padding='same', activation='relu',kernel_regularizer=regularizers.l2(0.015), input_shape=(80,80,1)))\n",
        "Boat_Detection.add(BatchNormalization())\n",
        "\n",
        "\n",
        "Boat_Detection.add(Conv2D(64, (5,5), strides=(1, 1), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.015)))\n",
        "Boat_Detection.add(Dropout(0.6))\n",
        "Boat_Detection.add(BatchNormalization())\n",
        "Boat_Detection.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "Boat_Detection.add(Conv2D(64, (3,3), strides=(1, 1), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
        "Boat_Detection.add(Dropout(0.7))\n",
        "Boat_Detection.add(BatchNormalization())\n",
        "Boat_Detection.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "Boat_Detection.add(GlobalAveragePooling2D())\n",
        "Boat_Detection.add(Dense(units=32, activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.02)))\n",
        "Boat_Detection.add(Dropout(0.8))\n",
        "Boat_Detection.add(Dense(units=2, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICoX3HTbFCnJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Add F1 score function to track it as well\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision_result = self.precision.result()\n",
        "        recall_result = self.recall.result()\n",
        "        return 2 * (precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goYHDU7or9JT"
      },
      "outputs": [],
      "source": [
        "Boat_Detection.compile(optimizer=AdamW(learning_rate=0.001,\n",
        "                                       weight_decay=2e-5,\n",
        "                                       clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall(), F1Score()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFm8ECB3r9JU"
      },
      "outputs": [],
      "source": [
        "Boat_Detection.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_KQKfBlr9JU"
      },
      "source": [
        "Incorporating K-folds + TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP0LviX0zGgw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "df = pd.read_csv('/content/labeled_images.csv')\n",
        "\n",
        "# KFold setup\n",
        "k = 5  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "image_directory = '/content/drive/MyDrive/train/'  # Path to the root directory containing subdirectories\n",
        "\n",
        "# ImageDataGenerator for training and validation\n",
        "datagen = ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    shear_range = 0.2,\n",
        "    channel_shift_range = 20.0,\n",
        "    brightness_range = [0.8, 1.2],\n",
        "    rescale=1./255\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "    print(f\"Starting fold {fold + 1}/{k}...\")\n",
        "\n",
        "    # Train and validation splits\n",
        "    train_df = df.iloc[train_idx]\n",
        "    val_df = df.iloc[val_idx]\n",
        "\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "    # Saving splits for debugging\n",
        "    train_df.to_csv(f'/content/train_labels_fold{fold + 1}.csv', index=False)\n",
        "    val_df.to_csv(f'/content/val_labels_fold{fold + 1}.csv', index=False)\n",
        "\n",
        "\n",
        "    # Generators for the current fold\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=image_directory,  # Root directory\n",
        "        x_col='filename',\n",
        "        y_col='label',\n",
        "        target_size=(80, 80),\n",
        "        batch_size=100,\n",
        "        class_mode='categorical',\n",
        "        color_mode='grayscale'\n",
        "    )\n",
        "\n",
        "    val_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=val_df,\n",
        "        directory=image_directory,  # Root directory\n",
        "        x_col='filename',\n",
        "        y_col='label',\n",
        "        target_size=(80, 80),\n",
        "        batch_size=100,\n",
        "        class_mode='categorical',\n",
        "        color_mode='grayscale'\n",
        "    )\n",
        "\n",
        "    print(f\"Fold {fold + 1} - Generators ready!\")\n",
        "\n",
        "    history = Boat_Detection.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=[learning_rate_callback],\n",
        "        epochs=10\n",
        "    )\n",
        "    Boat_Detection.save(f\"/content/fold_{fold+1}_model.h5\")\n",
        "    print(f\"Fold {fold + 1} - Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for folds overlapping"
      ],
      "metadata": {
        "id": "ZmjNvItDkt0w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm0MGvCoHvtM"
      },
      "outputs": [],
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "    train_filenames = set(df.iloc[train_idx]['filename'])\n",
        "    val_filenames = set(df.iloc[val_idx]['filename'])\n",
        "\n",
        "    # Check for overlap\n",
        "    overlap = train_filenames.intersection(val_filenames)\n",
        "    if overlap:\n",
        "        print(f\"Warning: The following files appear in both training and validation for fold {fold + 1}: {overlap}\")\n",
        "    else:\n",
        "        print(f\"Fold {fold + 1}: No overlap detected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NHH0k3My_mS"
      },
      "outputs": [],
      "source": [
        "evaluation = Boat_Detection.evaluate(val_generator)\n",
        "\n",
        "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vecd7ygRr9JU"
      },
      "outputs": [],
      "source": [
        "\"\"\" Old Model training \"\"\"\n",
        "# history = Boat_Detection.fit(\n",
        "#     train_generator,  # The generator for the training set\n",
        "#     class_weight=class_weights_dict,\n",
        "#     validation_data=test_generator,  # The generator for the validation set\n",
        "#     callbacks=[learning_rate_callback, warmup_lr_scheduler],\n",
        "#     batch_size = 256,\n",
        "#     epochs=10  # Or any number of epochs you choose\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRBUQ-6dvQcA"
      },
      "source": [
        "# PREDICTIONS VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "guQjjYjJNZod"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Predict on validation data (generated by val_generator)\n",
        "val_preds = Boat_Detection.predict(val_generator, verbose=1)\n",
        "\n",
        "# Get the true labels from the validation set\n",
        "val_true_labels = val_df['label'].replace({'ship': 1, 'no_ship': 0}).values  # Ensure correct encoding\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = np.argmax(val_preds, axis=-1)  # For multi-class (the predicted class index)\n",
        "\n",
        "# Compare predicted labels with actual labels\n",
        "mispredicted_indices = np.where(predicted_labels != val_true_labels)[0]\n",
        "\n",
        "# Display mispredicted images\n",
        "num_mispredicted = len(mispredicted_indices)\n",
        "print(f\"Total mispredicted images: {num_mispredicted}\")\n",
        "\n",
        "# Show mispredicted images\n",
        "for i in range(min(num_mispredicted, 10)):  # Show at most 5 mispredicted images\n",
        "    idx = mispredicted_indices[i]\n",
        "\n",
        "    # Get the mispredicted image\n",
        "    image_path = '/content/drive/MyDrive/train/' + val_df.iloc[idx]['filename']\n",
        "    img = plt.imread(image_path)  # Assuming the path is correct and image is accessible\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img, cmap='gray')  # Show the image (grayscale)\n",
        "    true_label = val_true_labels[idx]\n",
        "    pred_label = predicted_labels[idx]\n",
        "\n",
        "    # Plot title showing the true and predicted labels\n",
        "    plt.title(f\"True: {true_label}, Pred: {pred_label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DD3CPtyBTQF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Predict on validation data (generated by val_generator)\n",
        "val_preds = Boat_Detection.predict(val_generator, verbose=1)\n",
        "\n",
        "# Get the true labels from the validation set\n",
        "class_mapping = {'ship': 1, 'no_ship': 0}  # Adjust according to your classes\n",
        "val_true_labels = val_df['label'].replace(class_mapping).values\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = np.argmax(val_preds, axis=-1)\n",
        "\n",
        "# Identify correctly predicted indices for each class\n",
        "correct_indices_ship = np.where((predicted_labels == val_true_labels) & (val_true_labels == class_mapping['ship']))[0]\n",
        "correct_indices_no_ship = np.where((predicted_labels == val_true_labels) & (val_true_labels == class_mapping['no_ship']))[0]\n",
        "\n",
        "# Sample images for visualization (up to a limit)\n",
        "max_images_per_class = 3\n",
        "correct_ship_sample = correct_indices_ship[:max_images_per_class]\n",
        "correct_no_ship_sample = correct_indices_no_ship[:max_images_per_class]\n",
        "\n",
        "# Combine samples\n",
        "selected_indices = np.concatenate((correct_ship_sample, correct_no_ship_sample))\n",
        "\n",
        "# Map numeric labels back to class names\n",
        "inverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "# Define grid dimensions\n",
        "grid_rows = 2  # Adjust rows as needed\n",
        "grid_cols = max_images_per_class  # Set columns based on the sample size\n",
        "\n",
        "# Initialize the plot\n",
        "fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(10, 6))\n",
        "fig.subplots_adjust(hspace=0.25, wspace=0.25)\n",
        "\n",
        "# Display images\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < len(selected_indices):\n",
        "        idx = selected_indices[i]\n",
        "\n",
        "        # Get the image path\n",
        "        image_path = os.path.join('/content/drive/MyDrive/train/', val_df.iloc[idx]['filename'])\n",
        "\n",
        "        # Check if the file exists\n",
        "        if os.path.exists(image_path):\n",
        "            img = plt.imread(image_path)  # Load the image\n",
        "\n",
        "            # Get true and predicted labels\n",
        "            true_label = inverse_class_mapping[val_true_labels[idx]]\n",
        "            pred_label = inverse_class_mapping[predicted_labels[idx]]\n",
        "\n",
        "            # Display the image\n",
        "            ax.imshow(img, cmap='gray')  # Adjust colormap if necessary\n",
        "            ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=10)\n",
        "            ax.axis('off')  # Hide axes\n",
        "        else:\n",
        "            ax.set_title(\"Image not found\", fontsize=10)\n",
        "            ax.axis('off')\n",
        "    else:\n",
        "        ax.axis('off')  # Hide unused axes\n",
        "\n",
        "# Show the grid\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJBmKqP4zDXL"
      },
      "outputs": [],
      "source": [
        "\"\"\" Old version of prediction visualization \"\"\"\n",
        "\n",
        "def predict_and_plot(image_path, model, tile_size, scale_factor=1):\n",
        "    # Load the image in grayscale\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        print(f\"Error: Unable to load image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Optionally resize the image (for better visualization)\n",
        "    if scale_factor > 1:\n",
        "        image = resize_image(image, scale_factor)\n",
        "\n",
        "    # Split the image into tiles\n",
        "    tiles, positions = split_image_into_tiles(image, tile_size)\n",
        "\n",
        "    # Preprocess the tiles\n",
        "    preprocessed_tiles = preprocess_tiles(tiles)\n",
        "\n",
        "    # Generate predictions\n",
        "    predictions = model.predict(preprocessed_tiles)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)  # Convert to class indices\n",
        "\n",
        "    # Map class indices to labels\n",
        "    class_labels = {0: 'no_ship', 1: 'ship'}\n",
        "    predicted_labels = [class_labels[label] for label in predicted_labels]\n",
        "\n",
        "    # Adjust number of rows and columns for better visualization\n",
        "    num_rows = len(tiles) // 5 + (1 if len(tiles) % 5 != 0 else 0)  # 5 tiles per row\n",
        "    num_cols = min(len(tiles), 5)  # Max 5 tiles in one row\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3))  # Adjust figure size dynamically\n",
        "    axes = axes.flatten()\n",
        "    for idx, (tile, label) in enumerate(zip(tiles, predicted_labels)):\n",
        "        ax = axes[idx]\n",
        "        ax.imshow(tile, cmap='gray')\n",
        "        ax.axis('off')\n",
        "        ax.set_title(label, fontsize=12)\n",
        "\n",
        "    # Hide any unused subplot axes\n",
        "    for ax in axes[len(tiles):]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "uploaded_images = [os.path.join(upload_directory, img) for img in os.listdir(upload_directory) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "for image_path in uploaded_images:\n",
        "    predict_and_plot(image_path, model, tile_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL2Yv-wZ1pCo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths\n",
        "model_path = '/content/fold_5_model.h5'\n",
        "upload_directory = '/content/1/'\n",
        "model = load_model(model_path)\n",
        "\n",
        "tile_size = 80\n",
        "\n",
        "def count_full_tiles(image, tile_size):\n",
        "    h, w = image.shape  # Get image height and width\n",
        "    num_tiles_y = h // tile_size  # Full tiles along height\n",
        "    num_tiles_x = w // tile_size  # Full tiles along width\n",
        "    return num_tiles_y * num_tiles_x  # Total number of full tiles\n",
        "\n",
        "def split_image_into_tiles_exclude_borders(image, tile_size):\n",
        "    tiles = []\n",
        "    positions = []\n",
        "    h, w = image.shape\n",
        "\n",
        "    num_tiles_y = h // tile_size\n",
        "    num_tiles_x = w // tile_size\n",
        "\n",
        "    for y in range(num_tiles_y):\n",
        "        for x in range(num_tiles_x):\n",
        "            start_y = y * tile_size\n",
        "            start_x = x * tile_size\n",
        "            tile = image[start_y:start_y+tile_size, start_x:start_x+tile_size]\n",
        "            tiles.append(tile)\n",
        "            positions.append((start_x, start_y))\n",
        "\n",
        "    return np.array(tiles), positions\n",
        "\n",
        "def preprocess_tiles(tiles):\n",
        "    # normalize pixel values\n",
        "    tiles = np.array(tiles).astype('float32') / 255.0\n",
        "    return np.expand_dims(tiles, axis=-1)  # channel dimension\n",
        "\n",
        "def predict_and_overlay_exclude_borders(image_path, model, tile_size):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        print(f\"Error: Unable to load image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    full_tile_count = count_full_tiles(image, tile_size)\n",
        "    print(f\"Number of full tiles: {full_tile_count}\")\n",
        "\n",
        "\n",
        "    color_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)  # Convert grayscale to BGR\n",
        "\n",
        "    tiles, positions = split_image_into_tiles_exclude_borders(image, tile_size)\n",
        "    preprocessed_tiles = preprocess_tiles(tiles)\n",
        "\n",
        "    predictions = model.predict(preprocessed_tiles)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Determine grid size\n",
        "    original_height, original_width = image.shape\n",
        "    num_rows = original_height // tile_size\n",
        "    num_cols = original_width // tile_size\n",
        "\n",
        "    for idx, (label, pos) in enumerate(zip(predicted_labels, positions)):\n",
        "        if label == 1:  # 'ship' label\n",
        "            x, y = pos\n",
        "            overlay = color_image.copy()\n",
        "            cv2.rectangle(overlay, (x, y), (x + tile_size, y + tile_size), (255, 0, 0), -1)  # Blue rectangle\n",
        "            alpha = 0.3\n",
        "            cv2.addWeighted(overlay, alpha, color_image, 1 - alpha, 0, color_image)  # Blend overlay\n",
        "\n",
        "    plt.figure(figsize=(num_cols * 2, num_rows * 2))\n",
        "    plt.imshow(cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for Matplotlib\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "uploaded_images = [os.path.join(upload_directory, img) for img in os.listdir(upload_directory) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "for image_path in uploaded_images:\n",
        "    predict_and_overlay_exclude_borders(image_path, model, tile_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "num_full_tiles = count_full_tiles(image, tile_size)\n",
        "print(f\"Number of full tiles: {num_full_tiles}\")\n",
        "\n",
        "# Hard coded\n",
        "\n",
        "num_mispredicted = 7\n",
        "\n",
        "TP=3\n",
        "FP=7\n",
        "FN=0\n",
        "\n",
        "\n",
        "pred_accuracy=(num_full_tiles-num_mispredicted)/num_full_tiles\n",
        "print(f\"Prediction Accuracy: {pred_accuracy}\")\n",
        "\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1_score}\")"
      ],
      "metadata": {
        "id": "PQtZV_eDmx_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkbuhA9UvUGz"
      },
      "source": [
        "# Predictions review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0J_xFjJAypK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Predict on validation data (generated by val_generator)\n",
        "val_preds = Boat_Detection.predict(val_generator, verbose=1)\n",
        "\n",
        "# Get the true labels from the validation set\n",
        "class_mapping = {'ship': 1, 'no_ship': 0}  # Adjust according to your classes\n",
        "val_true_labels = val_df['label'].replace(class_mapping).values\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = np.argmax(val_preds, axis=-1)\n",
        "\n",
        "# Identify mispredicted indices\n",
        "mispredicted_indices = np.where(predicted_labels == val_true_labels)[0]\n",
        "\n",
        "# Display number of mispredicted images\n",
        "num_mispredicted = len(mispredicted_indices)\n",
        "print(f\"Total mispredicted images: {num_mispredicted}\")\n",
        "\n",
        "# Map numeric labels back to class names\n",
        "inverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "# Define grid dimensions\n",
        "grid_rows = 2  # Number of rows in the grid\n",
        "grid_cols = 3  # Number of columns in the grid\n",
        "max_images = grid_rows * grid_cols\n",
        "\n",
        "# Initialize the plot\n",
        "fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(10, 6))\n",
        "fig.subplots_adjust(hspace=0.25, wspace=0.25)\n",
        "\n",
        "# Display mispredicted images\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < len(mispredicted_indices):\n",
        "        idx = mispredicted_indices[i]\n",
        "\n",
        "        # Get the mispredicted image path\n",
        "        image_path = os.path.join('/content/drive/MyDrive/train/', val_df.iloc[idx]['filename'])\n",
        "\n",
        "        # Check if the file exists to avoid errors\n",
        "        if os.path.exists(image_path):\n",
        "            img = plt.imread(image_path)  # Load the image\n",
        "\n",
        "            # Get true and predicted labels\n",
        "            true_label = inverse_class_mapping[val_true_labels[idx]]\n",
        "            pred_label = inverse_class_mapping[predicted_labels[idx]]\n",
        "\n",
        "            # Display the image\n",
        "            ax.imshow(img, cmap='gray')  # Adjust colormap if needed\n",
        "            ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=10)\n",
        "            ax.axis('off')  # Hide axes\n",
        "        else:\n",
        "            ax.set_title(\"Image not found\", fontsize=10)\n",
        "            ax.axis('off')  # Hide axes\n",
        "    else:\n",
        "        ax.axis('off')  # Hide unused axes\n",
        "\n",
        "# Show the grid\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTwen0yo5QY8"
      },
      "source": [
        "#KERAS TUNER Tuning single hyperparameters 1 by 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DRMeXTa5Wjz"
      },
      "outputs": [],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi6cs0w9iiot"
      },
      "outputs": [],
      "source": [
        "# Load labeled data from CSV\n",
        "df = pd.read_csv('/content/labeled_images.csv')\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df.to_csv('/content/train_labels.csv', index=False)\n",
        "val_df.to_csv('/content/val_labels.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout 1 tuning"
      ],
      "metadata": {
        "id": "03BvbIflmAe4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "166-xnCXOqIL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Dropout, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras import regularizers\n",
        "from keras_tuner import HyperModel, GridSearch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom F1 Score Metric\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision_result = self.precision.result()\n",
        "        recall_result = self.recall.result()\n",
        "        return 2 * (precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon())\n",
        "\n",
        "\n",
        "# HyperModel for Keras Tuner\n",
        "class BoatDetectionHyperModel(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, (7,7), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.015), input_shape=(80,80,1)))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.015)))\n",
        "        model.add(Dropout(hp.Float('dropout_1', 0.3, 0.8, step=0.1)))  # Tunable dropout 1\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "        model.add(Conv2D(64, (3,3), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.02)))\n",
        "        model.add(Dropout(0.7))  # Fixed dropout 2\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
        "        model.add(Dropout(0.8))  # Fixed dropout 3\n",
        "        model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=AdamW(learning_rate=0.001,\n",
        "                                       weight_decay=2e-5,\n",
        "                                       clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
        "        return model\n",
        "\n",
        "\n",
        "# Initialize HyperModel\n",
        "hypermodel = BoatDetectionHyperModel()\n",
        "\n",
        "# Define Tuner\n",
        "tuner = GridSearch(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='/content/drive/MyDrive/hyperparam_tuning5',\n",
        "    project_name='dropout_tuning'\n",
        ")\n",
        "\n",
        "# Load data using ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    shear_range = 0.2,\n",
        "    channel_shift_range = 20.0,\n",
        "    brightness_range = [0.8, 1.2],\n",
        "    rescale=1./255\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "image_directory = '/content/drive/MyDrive/train/'\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=image_directory,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(80, 80),\n",
        "    batch_size=100,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "val_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    directory=image_directory,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(80, 80),\n",
        "    batch_size=100,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "# Perform Hyperparameter Search\n",
        "tuner.search(train_generator, validation_data=val_generator, callbacks=[learning_rate_callback], epochs=15)\n",
        "\n",
        "# Get Best Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(f\"Best Dropout Rate: {best_hps.get('dropout_1')}\")\n",
        "\n",
        "# Visualize Results\n",
        "best_trials = tuner.oracle.get_best_trials(num_trials=10)\n",
        "dropouts = []\n",
        "accuracies = []\n",
        "\n",
        "for trial in best_trials:\n",
        "    dropouts.append(trial.hyperparameters.get('dropout_1'))\n",
        "    accuracies.append(trial.metrics.get_last_value('val_accuracy'))\n",
        "\n",
        "# Sort the dropouts and accuracies based on dropout values\n",
        "sorted_dropouts, sorted_accuracies = zip(*sorted(zip(dropouts, accuracies)))\n",
        "\n",
        "# Plotting the sorted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_dropouts, sorted_accuracies, marker='o')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Dropout Rate vs Validation Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout 2 tuning"
      ],
      "metadata": {
        "id": "Ba8Mow_smeNI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1XD2ytaSZ1K"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Dropout, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras import regularizers\n",
        "from keras_tuner import HyperModel, GridSearch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom F1 Score Metric\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision_result = self.precision.result()\n",
        "        recall_result = self.recall.result()\n",
        "        return 2 * (precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon())\n",
        "\n",
        "\n",
        "# HyperModel for Keras Tuner\n",
        "class BoatDetectionHyperModel2(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, (7,7), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.015), input_shape=(80,80,1)))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.015)))\n",
        "        model.add(Dropout(0.6)) # Tunable dropout 1\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "        model.add(Conv2D(64, (3,3), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.02)))\n",
        "        model.add(Dropout(hp.Float('dropout_2', 0.4, 0.9, step=0.1))) # Fixed dropout 2\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
        "        model.add(Dropout(0.8))  # Fixed dropout 3\n",
        "        model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=AdamW(learning_rate=0.001,\n",
        "                                       weight_decay=2e-5,\n",
        "                                       clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
        "        return model\n",
        "\n",
        "\n",
        "# Initialize HyperModel\n",
        "hypermodel = BoatDetectionHyperModel2()\n",
        "\n",
        "# Define Tuner\n",
        "tuner = GridSearch(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='/content/drive/MyDrive/hyperparam_tuning5',\n",
        "    project_name='dropout_tuning211'\n",
        ")\n",
        "\n",
        "# Load data using ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    shear_range = 0.2,\n",
        "    channel_shift_range = 20.0,\n",
        "    brightness_range = [0.8, 1.2],\n",
        "    rescale=1./255\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "image_directory = '/content/drive/MyDrive/train/'\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=image_directory,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(80, 80),\n",
        "    batch_size=100,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "val_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    directory=image_directory,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(80, 80),\n",
        "    batch_size=100,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "# Perform Hyperparameter Search\n",
        "tuner.search(train_generator, validation_data=val_generator, callbacks=[learning_rate_callback], epochs=15)\n",
        "\n",
        "# Get Best Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(f\"Best Dropout Rate: {best_hps.get('dropout_2')}\")\n",
        "\n",
        "# Visualize Results\n",
        "best_trials = tuner.oracle.get_best_trials(num_trials=10)\n",
        "dropouts = []\n",
        "accuracies = []\n",
        "\n",
        "for trial in best_trials:\n",
        "    dropouts.append(trial.hyperparameters.get('dropout_2'))\n",
        "    accuracies.append(trial.metrics.get_last_value('val_accuracy'))\n",
        "\n",
        "# Sort the dropouts and accuracies based on dropout values\n",
        "sorted_dropouts, sorted_accuracies = zip(*sorted(zip(dropouts, accuracies)))\n",
        "\n",
        "# Plotting the sorted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_dropouts, sorted_accuracies, marker='o')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Dropout Rate vs Validation Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout 3 tuning\n"
      ],
      "metadata": {
        "id": "ib4e2gxNrP0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gQ3Nc2IqkaE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Dropout, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras import regularizers\n",
        "from keras_tuner import HyperModel, GridSearch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom F1 Score Metric\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision_result = self.precision.result()\n",
        "        recall_result = self.recall.result()\n",
        "        return 2 * (precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon())\n",
        "\n",
        "\n",
        "# HyperModel for Keras Tuner\n",
        "class BoatDetectionHyperModel3(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, (7,7), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.015), input_shape=(80,80,1)))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.015)))\n",
        "        model.add(Dropout(0.6)) # Tunable dropout 1\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "        model.add(Conv2D(64, (3,3), strides=(1, 1), padding='same', activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(0.02)))\n",
        "        model.add(Dropout(0.8)) # Fixed dropout 2\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
        "        model.add(Dropout(hp.Float('dropout_3', 0.5, 1, step=0.1)))  # Fixed dropout 3\n",
        "        model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=AdamW(learning_rate=0.001,\n",
        "                                       weight_decay=2e-5,\n",
        "                                       clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
        "        return model\n",
        "\n",
        "\n",
        "# Initialize HyperModel\n",
        "hypermodel = BoatDetectionHyperModel3()\n",
        "\n",
        "# Define Tuner\n",
        "tuner = GridSearch(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='/content/drive/MyDrive/hyperparam_tuning5',\n",
        "    project_name='dropout_tuning31'\n",
        ")\n",
        "\n",
        "# Load data using ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    shear_range = 0.2,\n",
        "    channel_shift_range = 20.0,\n",
        "    brightness_range = [0.8, 1.2],\n",
        "    rescale=1./255\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "image_directory = '/content/drive/MyDrive/train/'\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=image_directory,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(80, 80),\n",
        "    batch_size=100,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "val_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    directory=image_directory,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(80, 80),\n",
        "    batch_size=100,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "# Perform Hyperparameter Search\n",
        "tuner.search(train_generator, validation_data=val_generator, callbacks=[learning_rate_callback], epochs=15)\n",
        "\n",
        "# Get Best Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(f\"Best Dropout Rate: {best_hps.get('dropout_3')}\")\n",
        "\n",
        "# Visualize Results\n",
        "best_trials = tuner.oracle.get_best_trials(num_trials=10)\n",
        "dropouts = []\n",
        "accuracies = []\n",
        "\n",
        "for trial in best_trials:\n",
        "    dropouts.append(trial.hyperparameters.get('dropout_3'))\n",
        "    accuracies.append(trial.metrics.get_last_value('val_accuracy'))\n",
        "\n",
        "# Sort the dropouts and accuracies based on dropout values\n",
        "sorted_dropouts, sorted_accuracies = zip(*sorted(zip(dropouts, accuracies)))\n",
        "\n",
        "# Plotting the sorted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_dropouts, sorted_accuracies, marker='o')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Dropout Rate vs Validation Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llipfpUAyeqd"
      },
      "source": [
        "# DEBUG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking directory connection"
      ],
      "metadata": {
        "id": "QI25q4jQnC7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoH0cSIjNIK0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory where your images are stored\n",
        "directory = '/content/drive/MyDrive/train/no_ships'\n",
        "\n",
        "# List all files in the directory and filter by image extensions (e.g., .jpg, .png)\n",
        "image_files = [f for f in os.listdir(directory) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "# Get the number of image files\n",
        "num_images = len(image_files)\n",
        "\n",
        "print(f\"Number of images in directory: {num_images}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deletion of excess images"
      ],
      "metadata": {
        "id": "1mhDAX7-m12S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QKBB0uIN3Q7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Path to the 'no_ships' directory\n",
        "no_ships_dir = \"/content/drive/MyDrive/train/no\n",
        "\n",
        "# List all files in the directory\n",
        "no_ships_images = os.listdir(no_ships_dir)\n",
        "\n",
        "# Randomly select 700 files to delete\n",
        "files_to_delete = random.sample(no_ships_images, 120)\n",
        "\n",
        "# Delete the selected files\n",
        "for file in files_to_delete:\n",
        "    file_path = os.path.join(no_ships_dir, file)\n",
        "    os.remove(file_path)\n",
        "    print(f\"Deleted: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "757d5N3eOArL"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.copytree(\"/content/drive/MyDrive/train/no_ships\", \"/content/drive/MyDrive/train/ns1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check raw predictions"
      ],
      "metadata": {
        "id": "J3Q1XwrbnPZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_YJR8inyirS"
      },
      "outputs": [],
      "source": [
        "# Predicting on test data and printing raw prediction values\n",
        "predictions = Boat_Detection.predict(val_generator)\n",
        "\n",
        "# Print first 10 predictions for inspection\n",
        "print(\"First 10 raw predictions:\", predictions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check missing images"
      ],
      "metadata": {
        "id": "3bGKVq-mnMK8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYMjYd9KylaA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv('/content/train_labels.csv')\n",
        "test_df = pd.read_csv('/content/test_labels.csv')\n",
        "\n",
        "# Check for missing files in 'ships' and 'no_ships'\n",
        "ship_folder = '/content/drive/MyDrive/train/ships'\n",
        "no_ship_folder = '/content/drive/MyDrive/train/no_ships'\n",
        "\n",
        "# List all files in the directories\n",
        "ship_images = os.listdir(ship_folder)\n",
        "no_ship_images = os.listdir(no_ship_folder)\n",
        "\n",
        "# Check if all ship images are in the CSV\n",
        "for img in train_df['filename']:\n",
        "    if img not in ship_images and img not in no_ship_images:\n",
        "        print(f\"Missing image in CSV: {img}\")\n",
        "\n",
        "# Repeat for test data if needed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7ucYRAE4xL8r",
        "UgshqN4oxRoH",
        "T-ZUe6fexrxR",
        "qJo6Sc3k1Yn0",
        "dkbuhA9UvUGz",
        "6iWq-aYNx38R",
        "zNr2RJuYrwql",
        "KMrZR6bErDBH",
        "j20Z-YlS-_HU",
        "td8H-6ijq2R0"
      ],
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}